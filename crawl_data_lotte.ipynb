{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb5fa9b4",
   "metadata": {},
   "source": [
    "# Setup and Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a94f4f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install crawl4ai aiohttp nest_asyncio pymongo pillow -q\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d923d6f0",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e213145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime, timezone\n",
    "from bson import ObjectId\n",
    "\n",
    "\n",
    "def get_current_mongo_date():\n",
    "    return {\"$date\": datetime.now(timezone.utc).isoformat(timespec='milliseconds').replace(\"+00:00\", \"Z\")}\n",
    "\n",
    "def get_slug(url):\n",
    "    match = re.search(r'category/([a-z0-9-]+)-c\\d+', url)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "def save_json(data, path):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def append_json_list(data, path):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"r+\", encoding=\"utf-8\") as f:\n",
    "            existing = json.load(f)\n",
    "            f.seek(0)\n",
    "            json.dump(existing + data, f, ensure_ascii=False, indent=2)\n",
    "    else:\n",
    "        save_json(data, path)\n",
    "\n",
    "def load_json_file(filepath):\n",
    "    if os.path.exists(filepath):\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            try:\n",
    "                return json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                return []\n",
    "    return []\n",
    "\n",
    "\n",
    "def save_json_file(filepath, data):\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def append_csv_row(writer, row):\n",
    "    writer.writerow(row)\n",
    "\n",
    "def find_or_create_document(name, data_list, json_path, csv_writer):\n",
    "    # Tìm tên có sẵn (so sánh lowercase, strip)\n",
    "    name_normalized = name.lower().strip()\n",
    "    for doc in data_list:\n",
    "        if doc['name'].lower().strip() == name_normalized:\n",
    "            return doc['_id']\n",
    "    \n",
    "    # Nếu không có, tạo mới\n",
    "    new_id = str(ObjectId())\n",
    "    now = get_current_mongo_date()\n",
    "    new_doc = {\n",
    "        \"_id\": {\"$oid\": new_id},\n",
    "        \"name\": name.strip(),\n",
    "        \"createdAt\": now,\n",
    "        \"updatedAt\": now\n",
    "    }\n",
    "    data_list.append(new_doc)\n",
    "    save_json_file(json_path, data_list)\n",
    "    \n",
    "    # Ghi vào CSV\n",
    "    append_csv_row(csv_writer, {\n",
    "        \"_id\": new_id,\n",
    "        \"name\": name.strip(),\n",
    "        \"createdAt\": now[\"$date\"],\n",
    "        \"updatedAt\": now[\"$date\"]\n",
    "    })\n",
    "\n",
    "    return {\"$oid\": new_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bc9084",
   "metadata": {},
   "source": [
    "# Crawl Root Parent Category Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "408315b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080\">INIT</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span><span style=\"color: #008080; text-decoration-color: #008080\">.... → Crawl4AI </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7</span><span style=\"color: #008080; text-decoration-color: #008080\">.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m4\u001b[0m\u001b[36m \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">FETCH</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">... ↓ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://www.lottemart.vn</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                                             |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.</span><span style=\"color: #008000; text-decoration-color: #008000\">79s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mFETCH\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m...\u001b[0m\u001b[32m ↓ \u001b[0m\u001b[4;32mhttps://www.lottemart.vn\u001b[0m\u001b[32m                                                                             |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m1.\u001b[0m\u001b[32m79s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">SCRAPE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">.. ◆ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://www.lottemart.vn</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                                             |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.</span><span style=\"color: #008000; text-decoration-color: #008000\">03s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mSCRAPE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m.. ◆ \u001b[0m\u001b[4;32mhttps://www.lottemart.vn\u001b[0m\u001b[32m                                                                             |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m0.\u001b[0m\u001b[32m03s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">COMPLETE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\"> ● </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://www.lottemart.vn</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                                             |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.</span><span style=\"color: #008000; text-decoration-color: #008000\">83s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m ● \u001b[0m\u001b[4;32mhttps://www.lottemart.vn\u001b[0m\u001b[32m                                                                             |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m1.\u001b[0m\u001b[32m83s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 27 parent categories\n",
      "Saved to: output/data/categories_data.json and output/data/categories_data.csv\n",
      "Closure table saved to: output/data/category_paths.json and output/data/category_paths.csv\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n",
    "from bson import ObjectId\n",
    "import re\n",
    "\n",
    "URL = \"https://www.lottemart.vn\"\n",
    "\n",
    "CATEGORIES_CSV_FILE = \"output/data/categories_data.csv\"\n",
    "CATEGORIES_DATA_FILE = \"output/data/categories_data.json\"\n",
    "CATEGORY_PATHS_JSON = \"output/data/category_paths.json\"\n",
    "CATEGORY_PATHS_CSV = \"output/data/category_paths.csv\"\n",
    "\n",
    "\n",
    "def get_slug(url):\n",
    "    match = re.search(r\"category/([a-z0-9-]+)-c\\d+\", url)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "\n",
    "# Hàm hỗ trợ lấy thời gian hiện tại theo định dạng MongoDB\n",
    "def get_current_mongo_date():\n",
    "    return {\"$date\": datetime.utcnow().isoformat(timespec=\"milliseconds\") + \"Z\"}\n",
    "\n",
    "\n",
    "# Ghi thêm vào file CSV (nếu chưa có thì tạo mới)\n",
    "def append_csv(data: list, file_path: str, fieldnames: list):\n",
    "    if not data:\n",
    "        return\n",
    "\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "\n",
    "    existing_records = set()\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            for row in reader:\n",
    "                if \"ancestor\" in row and \"descendant\" in row:\n",
    "                    existing_records.add((row[\"ancestor\"], row[\"descendant\"]))\n",
    "                elif \"_id\" in row:\n",
    "                    existing_records.add(row[\"_id\"])\n",
    "\n",
    "    new_rows = []\n",
    "    for row in data:\n",
    "        if \"ancestor\" in row and \"descendant\" in row:\n",
    "            key = (row[\"ancestor\"], row[\"descendant\"])\n",
    "            if key not in existing_records:\n",
    "                new_rows.append(row)\n",
    "        elif \"_id\" in row:\n",
    "            if row[\"_id\"] not in existing_records:\n",
    "                new_rows.append(row)\n",
    "\n",
    "    if not new_rows:\n",
    "        return\n",
    "\n",
    "    file_exists = os.path.exists(file_path)\n",
    "\n",
    "    with open(file_path, mode=\"a\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        for row in new_rows:\n",
    "            writer.writerow(row)\n",
    "\n",
    "\n",
    "async def crawl_root_parent_category_links(crawler):\n",
    "    config = CrawlerRunConfig(\n",
    "        css_selector=\".col-menu > ul > li > a\",\n",
    "    )\n",
    "\n",
    "    os.makedirs(\"output/links\", exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(CATEGORIES_DATA_FILE), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(CATEGORY_PATHS_JSON), exist_ok=True)\n",
    "\n",
    "    result = await crawler.arun(url=URL, config=config)\n",
    "\n",
    "    formatted_data = []\n",
    "    csv_data = []\n",
    "    closure_json_data = []\n",
    "    closure_csv_data = []\n",
    "\n",
    "    now = get_current_mongo_date()\n",
    "\n",
    "    if result.links and \"internal\" in result.links:\n",
    "        for link_obj in result.links[\"internal\"]:\n",
    "            href = link_obj.get(\"href\")\n",
    "            name = link_obj.get(\"text\", \"\").strip()\n",
    "            slug = get_slug(href)\n",
    "\n",
    "            if href and name and slug:\n",
    "                object_id = ObjectId()\n",
    "                object_id_str = str(object_id)\n",
    "\n",
    "                # Category document\n",
    "                doc = {\n",
    "                    \"_id\": {\"$oid\": object_id_str},\n",
    "                    \"href\": href,\n",
    "                    \"slug\": slug,\n",
    "                    \"name\": name,\n",
    "                    \"parentCategory\": None,\n",
    "                    \"createdAt\": now,\n",
    "                    \"updatedAt\": now,\n",
    "                }\n",
    "                formatted_data.append(doc)\n",
    "\n",
    "                csv_data.append(\n",
    "                    {\n",
    "                        \"_id\": object_id_str,\n",
    "                        \"name\": name,\n",
    "                        \"slug\": slug,\n",
    "                        \"parent_category\": \"\",\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                # Closure Table: Root categories chỉ có self-reference với depth = 0\n",
    "                closure_json_data.append(\n",
    "                    {\n",
    "                        \"ancestor\": {\"$oid\": object_id_str},\n",
    "                        \"descendant\": {\"$oid\": object_id_str},\n",
    "                        \"depth\": 0,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                closure_csv_data.append(\n",
    "                    {\"ancestor\": object_id_str, \"descendant\": object_id_str, \"depth\": 0}\n",
    "                )\n",
    "\n",
    "    # Ghi JSON gốc và cũng là file tổng\n",
    "    with open(CATEGORIES_DATA_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(formatted_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # Ghi thêm file JSON riêng nếu muốn tách riêng\n",
    "    with open(\"output/links/root_parent_cat_links.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(formatted_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # Ghi CSV categories\n",
    "    append_csv(\n",
    "        csv_data, CATEGORIES_CSV_FILE, [\"_id\", \"name\", \"slug\", \"parent_category\"]\n",
    "    )\n",
    "\n",
    "    # Ghi Closure Table files\n",
    "    with open(CATEGORY_PATHS_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(closure_json_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    append_csv(\n",
    "        closure_csv_data, CATEGORY_PATHS_CSV, [\"ancestor\", \"descendant\", \"depth\"]\n",
    "    )\n",
    "\n",
    "    print(f\"Collected {len(formatted_data)} parent categories\")\n",
    "    print(f\"Saved to: {CATEGORIES_DATA_FILE} and {CATEGORIES_CSV_FILE}\")\n",
    "    print(f\"Closure table saved to: {CATEGORY_PATHS_JSON} and {CATEGORY_PATHS_CSV}\")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    async with AsyncWebCrawler() as crawler:\n",
    "        await crawl_root_parent_category_links(crawler)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfc2998",
   "metadata": {},
   "source": [
    "# Crawl All Category Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6855d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080\">INIT</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span><span style=\"color: #008080; text-decoration-color: #008080\">.... → Crawl4AI </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7</span><span style=\"color: #008080; text-decoration-color: #008080\">.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m4\u001b[0m\u001b[36m \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[done] All root categories are fully crawled to leaf.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import signal\n",
    "import time\n",
    "from datetime import datetime, timezone as UTC\n",
    "from bson import ObjectId\n",
    "from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig\n",
    "\n",
    "# ====== Files (giữ tên/đường dẫn như notebook hiện có) ======\n",
    "ROOT_PARENT_CATEGORIES_FILE = \"output/links/root_parent_cat_links.json\"\n",
    "CATEGORIES_DATA_FILE        = \"output/data/categories_data.json\"\n",
    "LAST_CATEGORIES_FILE        = \"output/links/last_categories_data.json\"\n",
    "CATEGORIES_CSV_FILE         = \"output/data/categories_data.csv\"\n",
    "CATEGORY_PATHS_JSON         = \"output/data/category_paths.json\"\n",
    "CATEGORY_PATHS_CSV          = \"output/data/category_paths.csv\"\n",
    "\n",
    "# ====== Checkpoint/Progress files ======\n",
    "PROGRESS_FILE              = \"output/links/crawl_progress_point.json\"\n",
    "COMPLETED_ROOTS_FILE       = \"output/links/completed_roots.json\"\n",
    "\n",
    "# ====== Browser config ======\n",
    "browser_config = BrowserConfig(headless=True, verbose=True)\n",
    "\n",
    "# ====== Tham số watchdog ======\n",
    "STALL_SECONDS   = 120          # nếu quá thời gian này không có tiến triển -> checkpoint & dừng\n",
    "SLEEP_BEFORE_EXIT = 5          # ngủ vài giây trước khi thoát để tránh spam\n",
    "PRINT_EVERY     = 50           # in log mỗi khi ghi thêm ~50 categories\n",
    "\n",
    "def load_json(filepath):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def append_csv(data, path, fieldnames):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    \n",
    "    if not data:\n",
    "        return\n",
    "\n",
    "    write_header = not os.path.exists(path)\n",
    "    with open(path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        if write_header:\n",
    "            writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "def get_selector(level, has_children):\n",
    "    if level == 0:\n",
    "        return (\n",
    "            \".aside-left.offcanvas-body > .f-item.opened .all-cates > li.sub > a\"\n",
    "            if has_children else\n",
    "            \".aside-left.offcanvas-body > .f-item.opened .all-cates > li:not(.sub) > a\"\n",
    "        )\n",
    "    else:\n",
    "        return (\n",
    "            \".all-cates.ps-3 li.sub.active > .subcate > .inner > ul > li.sub > a\"\n",
    "            if has_children else\n",
    "            \".all-cates.ps-3 li.sub.active > .subcate > .inner > ul > li:not(.sub) > a\"\n",
    "        )\n",
    "\n",
    "async def crawl_category_links(crawler, url, level, has_children):\n",
    "    \"\"\"Crawl categories với selector cụ thể (có hoặc không có children)\"\"\"\n",
    "    selector = get_selector(level, has_children)\n",
    "    \n",
    "    # Wait condition: đợi menu category được load\n",
    "    wait_condition = \"\"\"() => {\n",
    "      const menu = document.querySelector('.aside-left.offcanvas-body .cate-menu-list');\n",
    "      return menu !== null && menu.children.length > 0;\n",
    "    }\"\"\"\n",
    "\n",
    "    config = CrawlerRunConfig(\n",
    "        css_selector=selector,\n",
    "        page_timeout=60000,  # 60 giây (đủ cho trang load)\n",
    "        wait_for=f\"js:{wait_condition}\",\n",
    "        delay_before_return_html=2.0,  # Đợi 2 giây sau khi condition đạt để đảm bảo menu render xong\n",
    "        stream=False,  # Không cần stream cho crawl links đơn giản\n",
    "        remove_overlay_elements=True,  # Loại bỏ popup/overlay có thể che menu\n",
    "        js_code=[\n",
    "            # Scroll để trigger lazy load nếu có\n",
    "            \"window.scrollTo(0, 300);\",\n",
    "            \"await new Promise(r => setTimeout(r, 500));\",\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        result = await crawler.arun(url=url, config=config)\n",
    "        \n",
    "        links = []\n",
    "        if result.links and 'internal' in result.links:\n",
    "            for link_obj in result.links['internal']:\n",
    "                href = link_obj.get(\"href\")\n",
    "                name = link_obj.get(\"text\", \"\").strip()\n",
    "                if href and name:\n",
    "                    links.append({\"name\": name, \"href\": href})\n",
    "        \n",
    "        return links\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error crawling {url} at level {level}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def _oid_to_str(_id):\n",
    "    \"\"\"Chuyển _id có thể là dict {'$oid': ...} hoặc ObjectId hoặc str -> str.\"\"\"\n",
    "    if isinstance(_id, dict) and \"$oid\" in _id:\n",
    "        return _id[\"$oid\"]\n",
    "    if isinstance(_id, ObjectId):\n",
    "        return str(_id)\n",
    "    return str(_id)\n",
    "\n",
    "def _ensure_files():\n",
    "    for p in [\n",
    "        ROOT_PARENT_CATEGORIES_FILE, CATEGORIES_DATA_FILE, LAST_CATEGORIES_FILE,\n",
    "        CATEGORIES_CSV_FILE, CATEGORY_PATHS_JSON, CATEGORY_PATHS_CSV\n",
    "    ]:\n",
    "        os.makedirs(os.path.dirname(p), exist_ok=True)\n",
    "\n",
    "    # Tập tin completed_roots/progress có thể chưa tồn tại\n",
    "    if not os.path.exists(COMPLETED_ROOTS_FILE):\n",
    "        save_json([], COMPLETED_ROOTS_FILE)\n",
    "\n",
    "def _load_completed_roots():\n",
    "    try:\n",
    "        return set(load_json(COMPLETED_ROOTS_FILE) or [])\n",
    "    except Exception:\n",
    "        return set()\n",
    "\n",
    "def _save_completed_roots(s):\n",
    "    save_json(sorted(list(s)), COMPLETED_ROOTS_FILE)\n",
    "\n",
    "def _load_progress():\n",
    "    if not os.path.exists(PROGRESS_FILE):\n",
    "        return None\n",
    "    try:\n",
    "        return load_json(PROGRESS_FILE)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _save_progress(state):\n",
    "    save_json(state, PROGRESS_FILE)\n",
    "    print(f\"[checkpoint] Saved progress to {PROGRESS_FILE} (root_idx={state.get('root_idx')}, stack={len(state.get('stack', []))})\")\n",
    "\n",
    "def _clear_progress():\n",
    "    if os.path.exists(PROGRESS_FILE):\n",
    "        os.remove(PROGRESS_FILE)\n",
    "\n",
    "def _mk_category_doc(href, name, slug, parent_oid):\n",
    "    now = get_current_mongo_date()\n",
    "    oid = ObjectId()\n",
    "    return {\n",
    "        \"_id\": {\"$oid\": str(oid)},\n",
    "        \"href\": href,\n",
    "        \"slug\": slug,\n",
    "        \"name\": name,\n",
    "        \"parentCategory\": {\"$oid\": parent_oid} if parent_oid else None,\n",
    "        \"createdAt\": now,\n",
    "        \"updatedAt\": now,\n",
    "    }\n",
    "\n",
    "def _mk_closure_rows(parent_closures, child_oid):\n",
    "    \"\"\"\n",
    "    Từ parent_closures (danh sách dict {'ancestor': {'$oid': ...}, 'depth': int})\n",
    "    tạo ra closure mới cho child: tất cả depth +1, kèm self-depth=0.\n",
    "    Trả về (closures_json, closures_csv_rows)\n",
    "    \"\"\"\n",
    "    closures_json = []\n",
    "    closures_csv = []\n",
    "\n",
    "    # self reference depth=0\n",
    "    closures_json.append({\"ancestor\": {\"$oid\": child_oid}, \"descendant\": {\"$oid\": child_oid}, \"depth\": 0})\n",
    "    closures_csv.append({\"ancestor\": child_oid, \"descendant\": child_oid, \"depth\": 0})\n",
    "\n",
    "    # kế thừa từ parent, tăng depth\n",
    "    for c in parent_closures:\n",
    "        anc = _oid_to_str(c[\"ancestor\"][\"$oid\"]) if isinstance(c[\"ancestor\"], dict) else _oid_to_str(c[\"ancestor\"])\n",
    "        depth = int(c.get(\"depth\", 0)) + 1\n",
    "        closures_json.append({\"ancestor\": {\"$oid\": anc}, \"descendant\": {\"$oid\": child_oid}, \"depth\": depth})\n",
    "        closures_csv.append({\"ancestor\": anc, \"descendant\": child_oid, \"depth\": depth})\n",
    "    return closures_json, closures_csv\n",
    "\n",
    "def _now_iso():\n",
    "    return datetime.now(UTC).isoformat()\n",
    "\n",
    "async def crawl_all_categories_with_checkpoint():\n",
    "    _ensure_files()\n",
    "\n",
    "    if not os.path.exists(ROOT_PARENT_CATEGORIES_FILE):\n",
    "        print(f\"[error] Missing root categories: {ROOT_PARENT_CATEGORIES_FILE}\")\n",
    "        return\n",
    "\n",
    "    # 1) Tải danh sách root\n",
    "    root_categories = load_json(ROOT_PARENT_CATEGORIES_FILE) or []\n",
    "    if not root_categories:\n",
    "        print(\"[warn] No root categories found.\")\n",
    "        return\n",
    "\n",
    "    # 2) Khôi phục checkpoint nếu có\n",
    "    progress = _load_progress()\n",
    "    completed_roots = _load_completed_roots()\n",
    "\n",
    "    if progress is None:\n",
    "        # Khởi tạo state mới\n",
    "        start_root_idx = 0\n",
    "        stack = []   # mỗi item: {\"id\",\"href\",\"name\",\"level\",\"closures\":[...]}\n",
    "        seen = set() # tránh lặp trong 1 phiên\n",
    "        categories_written = 0\n",
    "        last_progress_ts = time.time()\n",
    "    else:\n",
    "        start_root_idx     = int(progress.get(\"root_idx\", 0))\n",
    "        stack              = progress.get(\"stack\", [])\n",
    "        seen               = set(progress.get(\"seen_ids\", []))\n",
    "        categories_written = int(progress.get(\"categories_written\", 0))\n",
    "        last_progress_ts   = progress.get(\"last_progress_ts\", time.time())\n",
    "        if isinstance(last_progress_ts, str):\n",
    "            # nếu lưu chuỗi, convert thành epoch  (safe fallback)\n",
    "            try:\n",
    "                last_progress_ts = datetime.fromisoformat(last_progress_ts.replace(\"Z\", \"+00:00\")).timestamp()\n",
    "            except Exception:\n",
    "                last_progress_ts = time.time()\n",
    "        print(f\"[resume] Resuming at root_idx={start_root_idx}, stack={len(stack)}, seen={len(seen)}\")\n",
    "\n",
    "    # 3) Crawler 1 phiên\n",
    "    async with AsyncWebCrawler(config=browser_config) as crawler:\n",
    "        # Duyệt từng root\n",
    "        for root_idx in range(start_root_idx, len(root_categories)):\n",
    "            root_cat = root_categories[root_idx]\n",
    "\n",
    "            root_oid = _oid_to_str(root_cat[\"_id\"])\n",
    "            if root_oid in completed_roots:\n",
    "                continue\n",
    "\n",
    "            # Nếu stack trống (tức là mới root này) -> push root\n",
    "            if not stack:\n",
    "                stack = [{\n",
    "                    \"id\": root_oid,\n",
    "                    \"href\": root_cat.get(\"href\"),\n",
    "                    \"name\": root_cat.get(\"name\", \"Unknown\"),\n",
    "                    \"level\": 0,\n",
    "                    \"closures\": [{\"ancestor\": {\"$oid\": root_oid}, \"depth\": 0}],\n",
    "                }]\n",
    "\n",
    "            print(f\"[root {root_idx+1}/{len(root_categories)}] Start: {root_cat.get('name', 'Unknown')} | stack={len(stack)}\")\n",
    "\n",
    "            while stack:\n",
    "                # Watchdog: nếu stall -> checkpoint và thoát\n",
    "                if (time.time() - last_progress_ts) > STALL_SECONDS:\n",
    "                    state = {\n",
    "                        \"root_idx\": root_idx,          # vẫn đang ở root này\n",
    "                        \"stack\": stack,\n",
    "                        \"seen_ids\": sorted(list(seen)),\n",
    "                        \"categories_written\": categories_written,\n",
    "                        \"last_progress_ts\": _now_iso(),\n",
    "                    }\n",
    "                    _save_progress(state)\n",
    "                    print(f\"[stall] No progress > {STALL_SECONDS}s. Pausing run. (wrote={categories_written})\")\n",
    "                    await asyncio.sleep(SLEEP_BEFORE_EXIT)\n",
    "                    return\n",
    "\n",
    "                node = stack.pop()  # DFS\n",
    "                node_id   = node[\"id\"]\n",
    "                node_href = node[\"href\"]\n",
    "                node_name = node[\"name\"]\n",
    "                level     = node[\"level\"]\n",
    "                closures  = node[\"closures\"]\n",
    "\n",
    "                if not node_href:\n",
    "                    continue\n",
    "                if node_id in seen:\n",
    "                    continue\n",
    "                seen.add(node_id)\n",
    "\n",
    "                # Crawl con (có sub và không có sub)\n",
    "                children_with_sub    = await crawl_category_links(crawler, node_href, level=level, has_children=True)\n",
    "                children_without_sub = await crawl_category_links(crawler, node_href, level=level, has_children=False)\n",
    "\n",
    "                children = []\n",
    "                if children_with_sub:\n",
    "                    children.extend(children_with_sub)\n",
    "                if children_without_sub:\n",
    "                    children.extend(children_without_sub)\n",
    "\n",
    "                # Normalize children (loại None, trùng)\n",
    "                uniq = {}\n",
    "                for ch in children or []:\n",
    "                    href = ch.get(\"href\")\n",
    "                    name = ch.get(\"name\", \"\").strip()\n",
    "                    slug = get_slug(href) if href else None\n",
    "                    if href and name and slug:\n",
    "                        uniq[href] = (href, name, slug)\n",
    "\n",
    "                if not uniq:\n",
    "                    # Leaf\n",
    "                    append_json_list([{\n",
    "                        \"_id\": {\"$oid\": node_id},\n",
    "                        \"href\": node_href,\n",
    "                        \"name\": node_name,\n",
    "                    }], LAST_CATEGORIES_FILE)\n",
    "                    continue\n",
    "\n",
    "                # Có con -> tạo doc + closure cho từng child, append file\n",
    "                cat_rows_csv = []\n",
    "                cat_rows_json = []\n",
    "                closure_rows_json = []\n",
    "                closure_rows_csv  = []\n",
    "\n",
    "                pushed = 0\n",
    "                for href, (href, name, slug) in uniq.items():\n",
    "                    # Lập doc child\n",
    "                    doc = _mk_category_doc(href, name, slug, parent_oid=node_id)\n",
    "                    child_oid = _oid_to_str(doc[\"_id\"])\n",
    "\n",
    "                    # Closure cho child\n",
    "                    cjson, ccsv = _mk_closure_rows(closures, child_oid)\n",
    "                    closure_rows_json.extend(cjson)\n",
    "                    closure_rows_csv.extend(ccsv)\n",
    "\n",
    "                    # Lưu category\n",
    "                    cat_rows_json.append(doc)\n",
    "                    cat_rows_csv.append({\n",
    "                        \"_id\": child_oid, \"href\": href, \"slug\": slug, \"name\": name,\n",
    "                        \"parentCategory\": node_id,\n",
    "                        \"createdAt\": doc[\"createdAt\"][\"$date\"] if isinstance(doc[\"createdAt\"], dict) else doc[\"createdAt\"],\n",
    "                        \"updatedAt\": doc[\"updatedAt\"][\"$date\"] if isinstance(doc[\"updatedAt\"], dict) else doc[\"updatedAt\"],\n",
    "                    })\n",
    "\n",
    "                    # Push child vào stack để duyệt tiếp\n",
    "                    child_closures = [{\"ancestor\": {\"$oid\": child_oid}, \"depth\": 0}] + [\n",
    "                        {\"ancestor\": {\"$oid\": _oid_to_str(c[\"ancestor\"][\"$oid\"]) if isinstance(c[\"ancestor\"], dict) else _oid_to_str(c[\"ancestor\"])},\n",
    "                         \"depth\": int(c.get(\"depth\", 0)) + 1}\n",
    "                        for c in closures\n",
    "                    ]\n",
    "                    stack.append({\n",
    "                        \"id\": child_oid,\n",
    "                        \"href\": href,\n",
    "                        \"name\": name,\n",
    "                        \"level\": level + 1,\n",
    "                        \"closures\": child_closures,\n",
    "                    })\n",
    "                    pushed += 1\n",
    "\n",
    "                # Ghi file (streaming)\n",
    "                if cat_rows_json:\n",
    "                    append_json_list(cat_rows_json, CATEGORIES_DATA_FILE)\n",
    "                if cat_rows_csv:\n",
    "                    append_csv(cat_rows_csv, CATEGORIES_CSV_FILE,\n",
    "                               [\"_id\",\"href\",\"slug\",\"name\",\"parentCategory\",\"createdAt\",\"updatedAt\"])\n",
    "                if closure_rows_json:\n",
    "                    append_json_list(closure_rows_json, CATEGORY_PATHS_JSON)\n",
    "                if closure_rows_csv:\n",
    "                    append_csv(closure_rows_csv, CATEGORY_PATHS_CSV, [\"ancestor\",\"descendant\",\"depth\"])\n",
    "\n",
    "                categories_written += len(cat_rows_json)\n",
    "                last_progress_ts = time.time()\n",
    "                if categories_written % PRINT_EVERY == 0 and categories_written > 0:\n",
    "                    print(f\"[progress] wrote {categories_written} categories. stack={len(stack)}\")\n",
    "\n",
    "                # Cập nhật checkpoint nhẹ (tối thiểu) theo nhịp,\n",
    "                # tránh ghi quá nhiều: chỉ khi stack lớn hoặc vừa ghi nhiều\n",
    "                if categories_written % (PRINT_EVERY * 4) == 0 or len(stack) > 500:\n",
    "                    state = {\n",
    "                        \"root_idx\": root_idx,\n",
    "                        \"stack\": stack,\n",
    "                        \"seen_ids\": sorted(list(seen)),\n",
    "                        \"categories_written\": categories_written,\n",
    "                        \"last_progress_ts\": _now_iso(),\n",
    "                    }\n",
    "                    _save_progress(state)\n",
    "\n",
    "            # Hết stack => root hoàn tất\n",
    "            completed_roots.add(root_oid)\n",
    "            _save_completed_roots(completed_roots)\n",
    "            # Xóa progress vì root này xong rồi\n",
    "            _clear_progress()\n",
    "            print(f\"[root done] {root_cat.get('name','Unknown')}\")\n",
    "\n",
    "    # 4) Nếu đến đây nghĩa là toàn bộ root đã xong\n",
    "    _clear_progress()\n",
    "    print(\"[done] All root categories are fully crawled to leaf.\")\n",
    "\n",
    "# ——— Entry Point ———\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        asyncio.run(crawl_all_categories_with_checkpoint())\n",
    "    except KeyboardInterrupt:\n",
    "        # Nếu người dùng bấm STOP, vẫn cố lưu checkpoint tối thiểu\n",
    "        print(\"\\n[interrupt] KeyboardInterrupt detected, saving checkpoint...\")\n",
    "        # Không có state cục bộ ở đây; nếu muốn “bắt” state live,\n",
    "        # có thể chuyển state ra phạm vi toàn cục. Ở phiên bản này,\n",
    "        # rely vào checkpoint đã lưu theo nhịp ở trên.\n",
    "        time.sleep(SLEEP_BEFORE_EXIT)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa57357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tổng số dòng (bao gồm header) = 420\n",
      "Số categories (bỏ header)    = 419\n"
     ]
    }
   ],
   "source": [
    "# Kiểm tra file CSV (đếm chay 25/9/2025 là 296 categories)\n",
    "count = 0\n",
    "with open(\"output/data/categories_data.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for _ in f:\n",
    "        count += 1\n",
    "\n",
    "print(\"Tổng số dòng (bao gồm header) =\", count)\n",
    "print(\"Số categories (bỏ header)    =\", count - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd35d3ff",
   "metadata": {},
   "source": [
    "# Crawl All Product Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a359c7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import json\n",
    "from urllib.parse import urlparse\n",
    "from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode, BrowserConfig\n",
    "\n",
    "LAST_CATEGORIES_FILE = 'output/links/last_categories_data.json'\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "async def crawl_products_in_category(crawler, category_doc):\n",
    "    category_url = category_doc.get('href')\n",
    "    category_id = category_doc.get('_id', {}).get('$oid')\n",
    "\n",
    "    if not category_url or not category_id:\n",
    "        print(\"[SKIPPED] Missing href or _id in category document.\")\n",
    "        return\n",
    "\n",
    "    js_code = \"\"\"\n",
    "    (function() {\n",
    "        if (!window._crawl4ai_scrollIntervalStarted) {\n",
    "            window._crawl4ai_scrollIntervalStarted = true;\n",
    "\n",
    "            window.scrollTo(0, document.body.scrollHeight);\n",
    "            console.log('Initial scroll executed.');\n",
    "\n",
    "            window._crawl4ai_scrollIntervalId = setInterval(() => {\n",
    "                const currentHeight = document.body.scrollHeight;\n",
    "                window.scrollTo(0, currentHeight);\n",
    "            }, 2000);\n",
    "        }\n",
    "    })();\n",
    "    \"\"\"\n",
    "\n",
    "    wait_condition = \"\"\"() => {\n",
    "        const totalElement = document.querySelector('.txt-number .red-color');\n",
    "        const totalText = totalElement ? totalElement.textContent.trim() : '0';\n",
    "        const totalExpected = Number(totalText) || 0;\n",
    "\n",
    "        const loadedItems = document.querySelectorAll('.proudct-list .col'); \n",
    "        const loadedCount = loadedItems.length;\n",
    "\n",
    "        if (totalExpected > 0 && loadedCount >= totalExpected) {\n",
    "            if (window._crawl4ai_scrollIntervalId) {\n",
    "                clearInterval(window._crawl4ai_scrollIntervalId);\n",
    "                delete window._crawl4ai_scrollIntervalId;\n",
    "            }\n",
    "            if (window._crawl4ai_scrollIntervalStarted) {\n",
    "                delete window._crawl4ai_scrollIntervalStarted;\n",
    "            }\n",
    "            return true;\n",
    "        }\n",
    "        return false;\n",
    "    }\"\"\"\n",
    "\n",
    "    config = CrawlerRunConfig(\n",
    "        cache_mode=CacheMode.DISABLED,\n",
    "        js_code=js_code,\n",
    "        # page_timeout=60 * 60 * 1000,\n",
    "        wait_for=f\"js:{wait_condition}\",\n",
    "        exclude_external_links=True,\n",
    "        \n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    retries = 0\n",
    "    while retries <= MAX_RETRIES:\n",
    "        try:\n",
    "            result = await crawler.arun(url=category_url, config=config)\n",
    "            await asyncio.sleep(2)\n",
    "\n",
    "            hrefs = []\n",
    "            if result.links and 'internal' in result.links:\n",
    "                for link_obj in result.links['internal']:\n",
    "                    href = link_obj.get('href')\n",
    "                    if href and '/product/' in href:\n",
    "                        hrefs.append(href)\n",
    "\n",
    "            print(f\"[OK] Crawled {len(hrefs)} product links from: {category_url}\")\n",
    "\n",
    "            # === Lưu vào thư mục riêng theo _id ===\n",
    "            category_output_dir = os.path.join('output/links/products', category_id)\n",
    "            os.makedirs(category_output_dir, exist_ok=True)\n",
    "\n",
    "            output_file_path = os.path.join(category_output_dir, 'product_links.json')\n",
    "            with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(hrefs, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "            return\n",
    "\n",
    "        except Exception as e:\n",
    "            retries += 1\n",
    "            print(f\"[Retry {retries}/{MAX_RETRIES}] Failed to crawl {category_url}: {e}\")\n",
    "            await asyncio.sleep(2)\n",
    "\n",
    "    print(f\"[FAILED] Giving up on {category_url} after {MAX_RETRIES} retries.\")\n",
    "\n",
    "browser_config = BrowserConfig(\n",
    "    headless=False,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    async with AsyncWebCrawler(config=browser_config) as crawler:\n",
    "        with open(LAST_CATEGORIES_FILE, 'r', encoding='utf-8') as f:\n",
    "            categories_data = json.load(f)\n",
    "\n",
    "        for category in categories_data:\n",
    "            await crawl_products_in_category(crawler, category)\n",
    "\n",
    "    print(\"[DONE] Finished crawling all categories.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2326787f",
   "metadata": {},
   "source": [
    "# Crawl Product Detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cf2f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "import asyncio\n",
    "from urllib.parse import urlparse\n",
    "from datetime import datetime\n",
    "from bson import ObjectId\n",
    "\n",
    "from crawl4ai import (\n",
    "    AsyncWebCrawler,\n",
    "    CrawlerRunConfig,\n",
    "    CacheMode,\n",
    "    MemoryAdaptiveDispatcher,\n",
    "    BrowserConfig,\n",
    ")\n",
    "from crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n",
    "\n",
    "# ====== PATHS ======\n",
    "BASE_PRODUCTS_LINKS_FOLDER = \"output/links/products\"  # folder chứa <category_id>/product_links.json\n",
    "BASE_OUTPUT_FOLDER         = \"output/data\"\n",
    "BRANDS_JSON_PATH           = os.path.join(BASE_OUTPUT_FOLDER, \"brands_data.json\")\n",
    "COUNTRIES_JSON_PATH        = os.path.join(BASE_OUTPUT_FOLDER, \"countries_of_origin.json\")\n",
    "PROGRESS_FILE              = os.path.join(BASE_OUTPUT_FOLDER, \"crawl_progress.json\")\n",
    "\n",
    "PRODUCTS_CSV_FILE          = os.path.join(BASE_OUTPUT_FOLDER, \"products_data.csv\")\n",
    "PRODUCTS_JSONL_FILE        = os.path.join(BASE_OUTPUT_FOLDER, \"products_data.jsonl\")\n",
    "SPECS_JSONL_FILE           = os.path.join(BASE_OUTPUT_FOLDER, \"specifications.jsonl\")\n",
    "\n",
    "# ====== RUNTIME CONFIG ======\n",
    "CHUNK_SIZE            = 50     # số URL xử lý mỗi batch (giảm nếu RAM thấp)\n",
    "CHECKPOINT_INTERVAL   = 25     # checkpoint sau mỗi N sản phẩm thành công\n",
    "STALL_SECONDS         = 180    # nếu không có tiến triển trong N giây -> checkpoint & dừng\n",
    "PAGE_TIMEOUT_MS       = 45000  # timeout mỗi trang\n",
    "DELAY_BEFORE_RETURN   = 1.2\n",
    "MAX_SESSIONS          = 3      # đồng thời (giảm về 1 nếu vẫn hết RAM)\n",
    "\n",
    "# ====== SCHEMA (tuỳ layout trang) ======\n",
    "product_schema = {\n",
    "    \"name\": \"Product Details\",\n",
    "    \"baseSelector\": \"div.product-custom.product-single\",\n",
    "    \"fields\": [\n",
    "        {\"name\": \"name\", \"selector\": \"h2.field-name\",  \"type\": \"text\"},\n",
    "        {\"name\": \"price\", \"selector\": \"div.field-price\",\"type\": \"text\"},\n",
    "        {\"name\": \"description\", \"selector\": \"div.short-desc\", \"type\": \"text\"},\n",
    "        {\n",
    "            \"name\": \"specifications\",\n",
    "            \"type\": \"list\",\n",
    "            \"selector\": \"table > tbody > tr\",\n",
    "            \"fields\": [\n",
    "                {\"name\": \"name\",  \"selector\": \"th\", \"type\": \"text\"},\n",
    "                {\"name\": \"value\", \"selector\": \"td\", \"type\": \"text\"},\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "\n",
    "# ====== UTILS ======\n",
    "def ensure_dirs():\n",
    "    os.makedirs(BASE_OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "def normalize_url(u):\n",
    "    try:\n",
    "        p = urlparse(u)\n",
    "        return f\"{p.scheme}://{p.netloc}{p.path}\".rstrip(\"/\")\n",
    "    except:\n",
    "        return u\n",
    "\n",
    "def extract_barcode(url):\n",
    "    path = urlparse(url).path\n",
    "    # ví dụ: /vi/product/sua-chua-xyz-123456.html -> tách \"-123456\" hoặc theo cấu trúc bạn đã dùng\n",
    "    parts = path.rstrip(\"/\").split(\"-\")\n",
    "    return parts[-2] if len(parts) >= 2 else None\n",
    "\n",
    "def clean_price(raw_price: str) -> int:\n",
    "    if not raw_price:\n",
    "        return 0\n",
    "    s = re.sub(r\"-\\d+%\", \"\", raw_price)\n",
    "    s = re.sub(r\"[^\\d]\", \"\", s)\n",
    "    return int(s) if s else 0\n",
    "\n",
    "def normalize_numeric_value(value_str):\n",
    "    if not value_str or not isinstance(value_str, str):\n",
    "        return value_str\n",
    "    s = re.sub(r\"[^\\d.]\", \"\", value_str)\n",
    "    try:\n",
    "        v = float(s)\n",
    "        return int(v) if v.is_integer() else float(f\"{v:.10f}\".rstrip(\"0\").rstrip(\".\"))\n",
    "    except:\n",
    "        return value_str\n",
    "\n",
    "def append_jsonl(path, obj):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def load_or_init_list(path):\n",
    "    data = load_json_file(path)\n",
    "    return data if isinstance(data, list) else []\n",
    "\n",
    "def find_or_create_document(name, data_list, json_path, csv_writer):\n",
    "    if not name or not name.strip():\n",
    "        return None\n",
    "    key = name.lower().strip()\n",
    "    for doc in data_list:\n",
    "        if doc.get(\"name\",\"\").lower().strip() == key:\n",
    "            return {\"$oid\": doc[\"_id\"][\"$oid\"]}\n",
    "\n",
    "    oid = str(ObjectId())\n",
    "    now = get_current_mongo_date()\n",
    "    doc = {\"_id\":{\"$oid\": oid}, \"name\": name.strip(), \"createdAt\": now, \"updatedAt\": now}\n",
    "    data_list.append(doc)\n",
    "    save_json_file(json_path, data_list)\n",
    "\n",
    "    if csv_writer is not None:\n",
    "        try:\n",
    "            csv_writer.writerow({\"_id\": oid, \"name\": name.strip()})\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] CSV write failed ({json_path}): {e}\")\n",
    "\n",
    "    return {\"$oid\": oid}\n",
    "\n",
    "# ====== PROGRESS (checkpoint) ======\n",
    "def load_progress():\n",
    "    dflt = {\n",
    "        \"processed_urls\": [],          # list[str] các URL đã xử lý (đã chuẩn hoá)\n",
    "        \"current_category\": None,      # category_id hiện tại\n",
    "        \"cursor\": 0,                   # index link đang xử lý trong category hiện tại\n",
    "        \"total_processed\": 0,\n",
    "        \"total_failed\": 0,\n",
    "    }\n",
    "    data = load_json_file(PROGRESS_FILE)\n",
    "    if not data:\n",
    "        return dflt\n",
    "    # đảm bảo kiểu dữ liệu\n",
    "    for k in dflt:\n",
    "        data.setdefault(k, dflt[k])\n",
    "    if not isinstance(data[\"processed_urls\"], list):\n",
    "        data[\"processed_urls\"] = []\n",
    "    if not isinstance(data[\"cursor\"], int):\n",
    "        data[\"cursor\"] = 0\n",
    "    if not isinstance(data[\"total_processed\"], int):\n",
    "        data[\"total_processed\"] = 0\n",
    "    if not isinstance(data[\"total_failed\"], int):\n",
    "        data[\"total_failed\"] = 0\n",
    "    return data\n",
    "\n",
    "def save_progress(pg):\n",
    "    save_json_file(PROGRESS_FILE, pg)\n",
    "\n",
    "# ====== PARSER ======\n",
    "def parse_extracted_to_docs(result, category_id, brands_writer, countries_writer, brands_data, countries_data):\n",
    "    \"\"\"\n",
    "    Nhận result từ arun_many (đã có .extracted_content), trả về:\n",
    "      - products_csv_row (dict)\n",
    "      - product_json (dict) -> để ghi vào JSONL\n",
    "      - specs_json (dict)   -> để ghi vào JSONL\n",
    "    \"\"\"\n",
    "    data = json.loads(result.extracted_content or \"[]\")\n",
    "    if not data:\n",
    "        return None, None, None\n",
    "\n",
    "    item = data[0]\n",
    "    barcode     = extract_barcode(result.url)\n",
    "    name        = item.get(\"name\", \"\") or \"\"\n",
    "    price       = clean_price(item.get(\"price\", \"\"))\n",
    "    description = item.get(\"description\", \"\") or \"\"\n",
    "    specs       = item.get(\"specifications\", []) or []\n",
    "\n",
    "    # tách brand & country_of_origin\n",
    "    brand_name = \"\"\n",
    "    country_name = \"\"\n",
    "    for s in specs:\n",
    "        sn = (s.get(\"name\") or \"\").lower().strip()\n",
    "        sv = (s.get(\"value\") or \"\")\n",
    "        if sn in [\"nhãn hiệu\", \"thương hiệu\", \"brand\", \"nhà sản xuất\"]:\n",
    "            brand_name = sv; continue\n",
    "        if sn in [\"nơi sản xuất\", \"xuất xứ\", \"quốc gia\", \"origin\", \"country of origin\"]:\n",
    "            country_name = sv; continue\n",
    "\n",
    "    brand_id = find_or_create_document(brand_name,  brands_data, BRANDS_JSON_PATH,   brands_writer)   if brand_name   else None\n",
    "    ctry_id  = find_or_create_document(country_name, countries_data, COUNTRIES_JSON_PATH, countries_writer) if country_name else None\n",
    "\n",
    "    # lọc specs còn lại + normalize\n",
    "    rest_specs = []\n",
    "    for s in specs:\n",
    "        sn = (s.get(\"name\") or \"\").lower().strip()\n",
    "        sv = s.get(\"value\")\n",
    "        if sn in [\"nhãn hiệu\", \"thương hiệu\", \"brand\", \"nhà sản xuất\", \"nơi sản xuất\", \"xuất xứ\", \"quốc gia\", \"origin\", \"country of origin\"]:\n",
    "            continue\n",
    "        rest_specs.append({\"name\": s.get(\"name\",\"\"), \"value\": normalize_numeric_value(sv)})\n",
    "\n",
    "    now = get_current_mongo_date()\n",
    "    pid = str(ObjectId())\n",
    "\n",
    "    # CSV row (lean fields)\n",
    "    csv_row = {\n",
    "        \"_id\": pid,\n",
    "        \"barcode\": barcode,\n",
    "        \"name\": name,\n",
    "        \"price\": price,\n",
    "        \"description\": description,\n",
    "        \"brand\": (brand_id or {}).get(\"$oid\"),\n",
    "        \"country_of_origin\": (ctry_id or {}).get(\"$oid\"),\n",
    "        \"category\": category_id,\n",
    "    }\n",
    "\n",
    "    product_json = {\n",
    "        \"_id\": {\"$oid\": pid},\n",
    "        \"barcode\": barcode,\n",
    "        \"name\": name,\n",
    "        \"price\": price,\n",
    "        \"description\": description,\n",
    "        \"brand\": brand_id,\n",
    "        \"countryOfOrigin\": ctry_id,\n",
    "        \"category\": {\"$oid\": category_id},\n",
    "        \"attributes\": rest_specs,\n",
    "        \"createdAt\": now,\n",
    "        \"updatedAt\": now,\n",
    "    }\n",
    "\n",
    "    specs_json = {\n",
    "        \"product\": {\"$oid\": pid},\n",
    "        \"specifications\": rest_specs\n",
    "    }\n",
    "\n",
    "    return csv_row, product_json, specs_json\n",
    "\n",
    "async def iter_arun_many(crawler, urls, config, dispatcher=None):\n",
    "    res = crawler.arun_many(urls=urls, config=config, dispatcher=dispatcher)\n",
    "    # Nếu là async generator (có __aiter__), duyệt trực tiếp\n",
    "    if hasattr(res, \"__aiter__\"):\n",
    "        async for r in res:\n",
    "            yield r\n",
    "    else:\n",
    "        # Nếu là coroutine -> await để lấy list, rồi duyệt for thường\n",
    "        results = await res\n",
    "        for r in results:\n",
    "            yield r\n",
    "\n",
    "# ====== CRAWL 1 CATEGORY (theo batch, streaming IO, checkpoint) ======\n",
    "async def crawl_products_for_category(category_id, products_writer, brands_writer, countries_writer,\n",
    "                                     brands_data, countries_data, progress):\n",
    "\n",
    "    product_links_path = os.path.join(BASE_PRODUCTS_LINKS_FOLDER, category_id, \"product_links.json\")\n",
    "    if not os.path.exists(product_links_path):\n",
    "        print(f\"[SKIP] Missing product_links.json for {category_id}\")\n",
    "        return\n",
    "\n",
    "    with open(product_links_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        all_links = json.load(f) or []\n",
    "\n",
    "    processed = set(progress[\"processed_urls\"])\n",
    "    links = [u for u in all_links if normalize_url(u) not in processed]\n",
    "\n",
    "    start_idx = 0\n",
    "    if progress[\"current_category\"] == category_id:\n",
    "        start_idx = min(progress[\"cursor\"], len(links))\n",
    "\n",
    "    print(f\"[CATEGORY] {category_id}: {len(links)} pending (cursor={start_idx})\")\n",
    "\n",
    "    for start in range(start_idx, len(links), CHUNK_SIZE):\n",
    "        batch = links[start:start+CHUNK_SIZE]\n",
    "\n",
    "        # checkpoint vị trí\n",
    "        progress[\"current_category\"] = category_id\n",
    "        progress[\"cursor\"] = start\n",
    "        save_progress(progress)\n",
    "\n",
    "        browser_config = BrowserConfig(headless=True, verbose=False)\n",
    "        run_config = CrawlerRunConfig(\n",
    "            cache_mode=CacheMode.ENABLED,\n",
    "            extraction_strategy=JsonCssExtractionStrategy(product_schema),\n",
    "            page_timeout=PAGE_TIMEOUT_MS,\n",
    "            delay_before_return_html=DELAY_BEFORE_RETURN,\n",
    "            stream=False,\n",
    "        )\n",
    "        dispatcher = MemoryAdaptiveDispatcher(max_session_permit=MAX_SESSIONS)\n",
    "\n",
    "        async with AsyncWebCrawler(config=browser_config) as crawler:\n",
    "            async for result in iter_arun_many(crawler, batch, run_config, dispatcher):\n",
    "                url_norm = normalize_url(result.url)\n",
    "                if result.success and result.extracted_content and result.extracted_content != \"[]\":\n",
    "                    rows = parse_extracted_to_docs(\n",
    "                        result,\n",
    "                        category_id,\n",
    "                        brands_writer,\n",
    "                        countries_writer,\n",
    "                        brands_data,\n",
    "                        countries_data\n",
    "                    )\n",
    "                    if rows:\n",
    "                        csv_row, product_json, specs_json = rows\n",
    "                        products_writer.writerow(csv_row)\n",
    "                        append_jsonl(PRODUCTS_JSONL_FILE, product_json)\n",
    "                        append_jsonl(SPECS_JSONL_FILE,  specs_json)\n",
    "\n",
    "                        progress[\"processed_urls\"].append(url_norm)\n",
    "                        progress[\"total_processed\"] += 1\n",
    "                        if (progress[\"total_processed\"] % CHECKPOINT_INTERVAL) == 0:\n",
    "                            save_progress(progress)\n",
    "                else:\n",
    "                    progress[\"total_failed\"] += 1\n",
    "\n",
    "\n",
    "        # checkpoint sau batch + dọn RAM\n",
    "        save_progress(progress)\n",
    "        import gc; gc.collect()\n",
    "        await asyncio.sleep(0.5)\n",
    "\n",
    "    print(f\"[DONE CAT] {category_id}\")\n",
    "\n",
    "# ====== MAIN ======\n",
    "async def main():\n",
    "    ensure_dirs()\n",
    "\n",
    "    # mở các file CSV/JSONL ở chế độ append (resume-friendly)\n",
    "    products_csv_exists = os.path.exists(PRODUCTS_CSV_FILE)\n",
    "    brands_json = load_or_init_list(BRANDS_JSON_PATH)\n",
    "    countries_json = load_or_init_list(COUNTRIES_JSON_PATH)\n",
    "\n",
    "    # liệt kê category\n",
    "    if not os.path.isdir(BASE_PRODUCTS_LINKS_FOLDER):\n",
    "        print(f\"Missing folder: {BASE_PRODUCTS_LINKS_FOLDER}\")\n",
    "        return\n",
    "    category_ids = [d for d in os.listdir(BASE_PRODUCTS_LINKS_FOLDER)\n",
    "                    if os.path.isdir(os.path.join(BASE_PRODUCTS_LINKS_FOLDER, d))]\n",
    "    if not category_ids:\n",
    "        print(\"No category folders found.\")\n",
    "        return\n",
    "\n",
    "    progress = load_progress()\n",
    "    print(f\"[RESUME] processed_urls={len(progress['processed_urls'])}, total_processed={progress['total_processed']}, total_failed={progress['total_failed']}\")\n",
    "\n",
    "    # Nếu có current_category, resume từ đó trong order category_ids\n",
    "    start_cat_idx = 0\n",
    "    if progress[\"current_category\"] and progress[\"current_category\"] in category_ids:\n",
    "        start_cat_idx = category_ids.index(progress[\"current_category\"])\n",
    "\n",
    "    with open(PRODUCTS_CSV_FILE, \"a\", encoding=\"utf-8\", newline=\"\") as f_products, \\\n",
    "         open(os.path.join(BASE_OUTPUT_FOLDER, \"brands_data.csv\"), \"a\", encoding=\"utf-8\", newline=\"\") as f_brands_csv, \\\n",
    "         open(os.path.join(BASE_OUTPUT_FOLDER, \"countries_of_origin.csv\"), \"a\", encoding=\"utf-8\", newline=\"\") as f_countries_csv:\n",
    "\n",
    "        products_writer = csv.DictWriter(\n",
    "            f_products,\n",
    "            fieldnames=[\"_id\",\"barcode\",\"name\",\"price\",\"description\",\"brand\",\"country_of_origin\",\"category\"],\n",
    "            quoting=csv.QUOTE_MINIMAL\n",
    "        )\n",
    "        if not products_csv_exists:\n",
    "            products_writer.writeheader()\n",
    "\n",
    "        brands_writer    = csv.DictWriter(f_brands_csv,    fieldnames=[\"_id\",\"name\"])\n",
    "        countries_writer = csv.DictWriter(f_countries_csv, fieldnames=[\"_id\",\"name\"])\n",
    "        # headers cho brands/countries nếu file trống\n",
    "        if f_brands_csv.tell() == 0:    brands_writer.writeheader()\n",
    "        if f_countries_csv.tell() == 0: countries_writer.writeheader()\n",
    "\n",
    "        # duyệt category\n",
    "        for cat_id in category_ids[start_cat_idx:]:\n",
    "            progress[\"current_category\"] = cat_id\n",
    "            progress[\"cursor\"] = 0\n",
    "            save_progress(progress)\n",
    "\n",
    "            await crawl_products_for_category(\n",
    "                category_id=cat_id,\n",
    "                products_writer=products_writer,\n",
    "                brands_writer=brands_writer,\n",
    "                countries_writer=countries_writer,\n",
    "                brands_data=brands_json,\n",
    "                countries_data=countries_json,\n",
    "                progress=progress,\n",
    "            )\n",
    "\n",
    "            # sau khi xong cat -> reset vị trí, lưu brands/countries\n",
    "            progress[\"current_category\"] = None\n",
    "            progress[\"cursor\"] = 0\n",
    "            save_progress(progress)\n",
    "            save_json_file(BRANDS_JSON_PATH,    brands_json)\n",
    "            save_json_file(COUNTRIES_JSON_PATH, countries_json)\n",
    "\n",
    "    print(f\"Total_processed: {progress['total_processed']}\")\n",
    "    print(f\"Total_failed:    {progress['total_failed']}\")\n",
    "    print(f\"Brands:          {len(brands_json)}\")\n",
    "    print(f\"Countries:       {len(countries_json)}\")\n",
    "    print(\"Done!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
